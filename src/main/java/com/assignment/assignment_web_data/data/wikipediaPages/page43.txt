Analysis of variance Analysis of variance ANOVA is a collection of statistical models used in order to analyze the differences between group means and their associated procedures such as variation among and between groups developed by Fisher In the ANOVA setting the observed variance in a particular variable is partitioned into components attributable to different sources of variation In its simplest form ANOVA provides a statistical test of whether or not the means of several groups are equal and therefore generalizes the t test to more than two groups As doing multiple would result in an increased chance of committing a statistical type I error ANOVAs are useful in comparing testing three or more means groups or variables for statistical significance Motivating example In the illustrations to the right each group is identified as X X In the first illustration we divide the dogs according to the product interaction of two binary groupings young vs old and vs thus group is young dogs group is young dogs Since the distributions of dog weight within each of the groups shown in blue has a large variance and since the means are very close across groups grouping dogs by these characteristics does not produce an effective way to explain the variation in dog weights knowing which group a dog is in does not allow us to make any reasonable statements as to what that dog weight is likely to be Thus this grouping fails to fit the distribution we are trying to explain An attempt to explain the weight distribution by grouping dogs as pet vs working breed and less athletic vs more athletic would probably be somewhat more successful fair fit The heaviest show dogs are likely to be big strong working breeds while breeds kept as pets tend to be smaller and thus lighter As shown by the second illustration the distributions have variances that are considerably smaller than in the first case and the means are more reasonably distinguishable However the significant overlap of distributions for example means that we can not reliably say that X and X are truly distinct it is perhaps reasonably likely that splitting dogs according to the flip of a coin by pure chance might produce distributions that look similar An attempt to explain weight by breed is likely to produce a very good fit All Chihuahuas are light and all St Bernards are heavy The difference in weights between Setters and Pointers does not justify separate breeds The analysis of variance provides the formal tools to justify these intuitive judgments A common use of the method is the analysis of experimental data or the development of models The method has some advantages over correlation not all of the data must be numeric and one result of the method is a judgment in the confidence in an explanatory relationship Background and terminology ANOVA is a particular form of statistical hypothesis testing heavily used in the analysis of experimental data A statistical hypothesis test is a method of making decisions using data A test result calculated from the null hypothesis and the sample is called statistically significant if it is deemed unlikely to have occurred by chance assuming the truth of the null hypothesis A statistically significant result when a probability is less than a threshold significance level justifies the rejection of the null hypothesis but only if the a priori probability of the null hypothesis is not high In the typical application of ANOVA the null hypothesis is that all groups are simply random samples of the same population For example when studying the effect of different treatments on similar samples of patients the null hypothesis would be that all treatments have the same effect perhaps none Rejecting the null hypothesis would imply that different treatments result in altered effects By construction hypothesis testing limits the rate of Type I errors false positives leading to false scientific claims to a significance level Experimenters also wish to limit Type II errors false negatives resulting in missed scientific discoveries The Type II error rate is a function of several things including sample size positively correlated with experiment cost significance level when the standard of proof is high the chances of overlooking a discovery are also high and effect size when the effect is obvious to the casual observer Type II error rates are low The terminology of ANOVA is largely from the statistical design of experiments The experimenter adjusts factors and measures responses in an attempt to determine an effect Factors are assigned to experimental units by a combination of randomization and blocking to ensure the validity of the results Blinding keeps the weighing impartial Responses show a variability that is partially the result of the effect and is partially random error ANOVA is the synthesis of several ideas and it is used for multiple purposes As a consequence it is difficult to define concisely or precisely Classical ANOVA for balanced data does three things at once In short ANOVA is a statistical tool used in several ways to develop and confirm an explanation for the observed data Additionally As a result ANOVA has long enjoyed the status of being the most used some would say abused statistical technique in psychological research ANOVA is probably the most useful technique in the field of statistical inference ANOVA is difficult to teach particularly for complex experiments with designs being notorious In some cases the proper application of the method is best determined by problem pattern recognition followed by the consultation of a classic authoritative test terms Condensed from the NIST Engineering Statistics handbook Section A Glossary of DOE Terminology Classes of models There are three classes of models used in the analysis of variance and these are outlined here models The model of analysis of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see if the response variable values change This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole models Random effects models are used when the treatments are not fixed This occurs when the various factor levels are sampled from a larger population Because the levels themselves are random variables some assumptions and the method of contrasting the treatments a generalization of simple differences differ from the model models A model contains experimental factors of both fixed and types with appropriately different interpretations and analysis for the two types Example Teaching experiments could be performed by a university department to find a good introductory textbook with each text considered a treatment The model would compare a list of candidate texts The model would determine whether important differences exist among a list of randomly selected texts The model would compare the fixed incumbent texts to randomly selected alternatives Defining fixed and random effects has proven elusive with competing definitions arguably leading toward a linguistic quagmire Assumptions of ANOVA The analysis of variance has been studied from several approaches the most common of which uses a linear model that relates the response to the treatments and blocks Note that the model is linear in parameters but may be nonlinear across factor levels Interpretation is easy when data is balanced across factors but much deeper understanding is needed for unbalanced data Textbook analysis using a normal distribution The analysis of variance can be presented in terms of a linear model which makes the following assumptions about the probability distribution of the responses The separate assumptions of the textbook model imply that the errors are independently identically and normally distributed for fixed effects models that is that the errors are independent and analysis In a randomized controlled experiment the treatments are randomly assigned to experimental units following the experimental protocol This randomization is objective and declared before the experiment is carried out The objective is used to test the significance of the null hypothesis following the ideas of Peirce and Ronald Fisher This analysis was discussed and developed by Francis Anscombe at Rothamsted Experimental Station and by Oscar Kempthorne at Iowa State University Kempthorne and his students make an assumption of unit treatment additivity which is discussed in the books of Kempthorne and David Cox additivity In its simplest form the assumption of additivity states that the observed response from experimental unit when receiving treatment can be written as the sum of the unit response and the that is The assumption of additivity implies that for every treatment the treatment have exactly the same effect on every experiment unit The assumption of unit treatment additivity usually can not be directly falsified according to Cox and Kempthorne However many consequences of additivity can be falsified For a randomized experiment the assumption of additivity implies that the variance is constant for all treatments Therefore by contraposition a necessary condition for additivity is that the variance is constant The use of unit treatment additivity and randomization is similar to the inference that is standard in survey sampling Derived linear model Kempthorne uses the and the assumption of unit treatment additivity to produce a derived linear model very similar to the textbook model discussed previously The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model according to approximation theorems and simulation studies However there are differences For example the analysis results in a small but strictly negative correlation between the observations In the analysis there is no assumption of a normal distribution and certainly no assumption of independence On the contrary the observations are dependent The analysis has the disadvantage that its exposition involves tedious algebra and extensive time Since the analysis is complicated and is closely approximated by the approach using a normal linear model most teachers emphasize the normal linear model approach Few statisticians object to analysis of balanced randomized experiments Statistical models for observational data However when applied to data from experiments or observational studies analysis lacks the warrant of randomization For observational data the derivation of confidence intervals must use subjective models as emphasized by Ronald Fisher and his followers In practice the estimates of from observational studies generally are often inconsistent In practice statistical models and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public Summary of assumptions The based ANOVA analysis assumes the independence normality and homogeneity of the variances of the residuals The analysis assumes only the homogeneity of the variances of the residuals as a consequence of additivity and uses the randomization procedure of the experiment Both these analyses require homoscedasticity as an assumption for the analysis and as a consequence of randomization and additivity for the analysis However studies of processes that change variances rather than means called dispersion effects have been successfully conducted using ANOVA There are no necessary assumptions for ANOVA in its full generality but the used for ANOVA hypothesis testing has assumptions and practical limitations which are of continuing interest Problems which do not satisfy the assumptions of ANOVA can often be transformed to satisfy the assumptions The property of additivity is not invariant under a change of scale so statisticians often use transformations to achieve additivity If the response variable is expected to follow a parametric family of probability distributions then the statistician may specify in the protocol for the experiment or observational study that the responses be transformed to stabilize the variance Also a statistician may specify that logarithmic transforms be applied to the responses which are believed to follow a multiplicative model According to Cauchy functional equation theorem the logarithm is the only continuous transformation that transforms real multiplication to addition Characteristics of ANOVA ANOVA is used in the analysis of comparative experiments those in which only the difference in outcomes is of interest The statistical significance of the experiment is determined by a ratio of two variances This ratio is independent of several possible alterations to the experimental observations Adding a constant to all observations does not alter significance Multiplying all observations by a constant does not alter significance So ANOVA statistical significance results are independent of constant bias and scaling errors as well as the units used in expressing observations In the era of mechanical calculation it was common to subtract a constant from all observations when equivalent to dropping leading digits to simplify data entry This is an example of data coding Logic of ANOVA The calculations of ANOVA can be characterized as computing a number of means and variances dividing two variances and comparing the ratio to a handbook value to determine statistical significance Calculating a treatment effect is then trivial the effect of any treatment is estimated by taking the difference between the mean of the observations which receive the treatment and the general mean Partitioning of the sum of squares ANOVA uses traditional standardized terminology The definitional equation of sample variance is where the divisor is called the degrees of freedom DF the summation is called the sum of squares SS the result is called the mean square MS and the squared terms are deviations from the sample mean ANOVA estimates sample variances a total variance based on all the observation deviations from the grand mean an error variance based on all the observation deviations from their appropriate treatment means and a treatment variance The treatment variance is based on the deviations of treatment means from the grand mean the result being multiplied by the number of observations in each treatment to account for the difference between the variance of observations and the variance of means The fundamental technique is a partitioning of the total sum of squares SS into components related to the effects used in the model For example the model for a simplified ANOVA with one type of treatment at different levels The number of degrees of freedom DF can be partitioned in a similar way one of these components that for error specifies a distribution which describes the associated sum of squares while the same is true for treatments if there is no treatment effect See also sum of squares The The is used for comparing the factors of the total deviation For example in or ANOVA statistical significance is tested for by comparing the F test statistic where MS is mean square number of treatments and total number of cases to the with degrees of freedom Using the is a natural candidate because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled distribution The expected value of F is where n is the treatment sample size which is for no treatment effect As values of F increase above the evidence is increasingly inconsistent with the null hypothesis Two apparent experimental methods of increasing F are increasing the sample size and reducing the error variance by tight experimental controls There are two methods of concluding the ANOVA hypothesis test both of which produce the same result The ANOVA is known to be nearly optimal in the sense of minimizing false negative errors for a fixed rate of false positive errors maximizing power for a fixed significance level For example to test the hypothesis that various medical treatments have exactly the same effect the closely approximate the permutation test The approximation is particularly close when the design is balanced Such permutation tests characterize tests with maximum power against all alternative hypotheses as observed by Rosenbaum The ANOVA F test of the that all treatments have exactly the same effect is recommended as a practical test because of its robustness against many alternative distributions Extended logic ANOVA consists of separable parts partitioning sources of variance and hypothesis testing can be used individually ANOVA is used to support other statistical tools Regression is first used to fit more complex models to data then ANOVA is used to compare models with the objective of selecting simple r models that adequately describe the data Such models could be fit without any reference to ANOVA but ANOVA tools could then be used to make some sense of the fitted models and to test hypotheses about batches of coefficients e think of the analysis of variance as a way of understanding and structuring multilevel models not as an alternative to regression but as a tool for summarizing complex inferences ANOVA for a single factor The simplest experiment suitable for ANOVA analysis is the completely randomized experiment with a single factor More complex experiments with a single factor involve constraints on randomization and include completely randomized blocks and Latin squares and variants squares The more complex experiments share many of the complexities of multiple factors A relatively complete discussion of the analysis models data summaries ANOVA table of the completely randomized experiment is available ANOVA for multiple factors ANOVA generalizes to the study of the effects of multiple factors When the experiment includes observations at all combinations of levels of each factor it is termed factorial Factorial experiments are more efficient than a series of single factor experiments and the efficiency grows as the number of factors increases Consequently factorial designs are heavily used The use of ANOVA to study the effects of multiple factors has a complication In a ANOVA with factors x y and z the ANOVA model includes terms for the main effects x y z and terms for interactions xy xz yz xyz All terms require hypothesis tests The proliferation of interaction terms increases the risk that some hypothesis test will produce a false positive by chance Fortunately experience says that high order interactions are rare The ability to detect interactions is a major advantage of multiple factor ANOVA Testing one factor at a time hides interactions but produces apparently inconsistent experimental results Caution is advised when encountering interactions Test interaction terms first and expand the analysis beyond ANOVA if interactions are found Texts vary in their recommendations regarding the continuation of the ANOVA procedure after encountering an interaction Interactions complicate the interpretation of experimental data Neither the calculations of significance nor the estimated treatment effects can be taken at face value A significant interaction will often mask the significance of main effects Graphical methods are recommended to enhance understanding Regression is often useful A lengthy discussion of interactions is available in Cox Some interactions can be removed by transformations while others can not A variety of techniques are used with multiple factor ANOVA to reduce expense One technique used in factorial designs is to minimize replication possibly no replication with support of analytical trickery and to combine groups when effects are found to be statistically or practically insignificant An experiment with many insignificant factors may collapse into one with a few factors supported by many replications Worked numeric examples Several fully worked numerical examples are available A simple case uses a single factor analysis A more complex case uses analysis Associated analysis Some analysis is required in support of the design of the experiment while other analysis is performed after changes in the factors are formally found to produce statistically significant changes in the responses Because experimentation is iterative the results of one experiment alter plans for following experiments Preparatory analysis The number of experimental units In the design of an experiment the number of experimental units is planned to satisfy the goals of the experiment Experimentation is often sequential Early experiments are often designed to provide estimates of treatment effects and of experimental error Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude in this case the number of experimental units is chosen so that the experiment is within budget and has adequate power among other goals Reporting sample size analysis is generally required in psychology Provide information on sample size and the process that led to sample size decisions The analysis which is written in the experimental protocol before the experiment is conducted is examined in grant applications and administrative review boards Besides the power analysis there are less formal methods for selecting the number of experimental units These include graphical methods based on limiting the probability of false negative errors graphical methods based on an expected variation increase above the residuals and methods based on achieving a desired confident interval Power analysis Power analysis is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design effect size in the population sample size and significance level Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true Effect size Several standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor s and the dependent variable or or the overall standardized difference of the complete model Standardized estimates facilitate comparison of findings across studies and disciplines However while standardized effect sizes are commonly used in much of the professional literature a measure of effect size that has immediately meaningful units may be preferable for reporting purposes Followup analysis It is always appropriate to carefully consider outliers They have a disproportionate impact on statistical conclusions and are often the result of errors Model confirmation It is prudent to verify that the assumptions of ANOVA have been met Residuals are examined or analyzed to confirm homoscedasticity and gross normality Residuals should have the appearance of zero mean normal distribution noise when plotted as a function of anything including time and modeled data values Trends hint at interactions among factors or among observations One rule of thumb If the largest standard deviation is less than twice the smallest standard deviation we can use methods based on the assumption of equal standard deviations and our results will still be approximately correct tests A statistically significant effect in ANOVA is often followed up with one or more different tests This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses tests are often distinguished in terms of whether they are planned a priori or post hoc Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data Often one of the treatments is none so the treatment group can act as a control Dunnett test a modification of the tests whether each of the other treatment groups has the same mean as the control Post hoc tests such as Tukey range test most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors Comparisons which are most commonly planned can be either simple or compound Simple comparisons compare one group mean with one other group mean Compound comparisons typically compare two sets of groups means where one set has two or more groups compare average group means of group A B and C with group D Comparisons can also look at tests of trend such as linear and quadratic relationships when the independent variable involves ordered levels Following ANOVA with tests has been criticized on several grounds There are many such tests in one table and recommendations regarding their use are vague or conflicting Study designs and ANOVAs There are several types of ANOVA Many statisticians base ANOVA on the design of the experiment especially on the protocol that specifies the random assignment of treatments to subjects the protocol description of the assignment mechanism should include a specification of the structure of the treatments and of any blocking It is also common to apply ANOVA to observational data using an appropriate statistical model Some popular designs use the following types of ANOVA ANOVA cautions Balanced experiments those with an equal sample size for each treatment are relatively easy to interpret Unbalanced experiments offer more complexity For single factor one way ANOVA the adjustment for unbalanced data is easy but the unbalanced analysis lacks both robustness and power For more complex designs the lack of balance leads to further complications The orthogonality property of main effects and interactions present in balanced data does not carry over to the unbalanced case This means that the usual analysis of variance techniques do not apply Consequently the analysis of unbalanced factorials is much more difficult than that for balanced designs In the general case The analysis of variance can also be applied to unbalanced data but then the sums of squares mean squares and will depend on the order in which the sources of variation are considered The simplest techniques for handling unbalanced data restore balance by either throwing out data or by synthesizing missing data More complex techniques use regression ANOVA is in part a significance test The American Psychological Association holds the view that simply reporting significance is insufficient and that reporting confidence bounds is preferred While ANOVA is conservative in maintaining a significance level against multiple comparisons in one dimension it is not conservative against comparisons in multiple dimensions Generalizations ANOVA is considered to be a special case of linear regression which in turn is a special case of the general linear model All consider the observations to be the sum of a model fit and a residual error to be minimized The Kruskal Wallis test and the Friedman test are nonparametric tests which do not rely on an assumption of normality History While the analysis of variance reached fruition in the century antecedents extend centuries into the past according to Stigler These include hypothesis testing the partitioning of sums of squares experimental techniques and the additive model Laplace was performing hypothesis testing in the The development of methods by Laplace and Gauss circa provided an improved method of combining observations over the existing practices of astronomy and geodesy It also initiated much study of the contributions to sums of squares Laplace soon knew how to estimate a variance from a residual rather than a total sum of squares By Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides Before astronomers had isolated observational errors resulting from reaction times the personal equation and had developed methods of reducing the errors The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong full factorial experimental methods to which randomization and blinding were soon added An eloquent explanation of the additive effects model was available in Sir Ronald Fisher introduced the term variance and proposed a formal analysis of variance in a article The Correlation Between Relatives on the Supposition of Mendelian Inheritance His first application of the analysis of variance was published in Analysis of variance became widely known after being included in Fisher book Statistical Methods for Research Workers Randomization models were developed by several researchers The first was published in Polish by Neyman in One of the attributes of ANOVA which ensured its early popularity was computational elegance The structure of the additive model allows solution for the additive coefficients by simple algebra rather than by matrix calculations In the era of mechanical calculators this simplicity was critical The determination of statistical significance also required access to tables of the F function which were supplied by early statistics texts 